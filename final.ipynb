{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the introduction to this notebook first...\n",
    "\n",
    "... or dive right in :)\n",
    "\n",
    "## Contents\n",
    "1. Reinforcement Learning \n",
    "2. Q-Learning\n",
    "3. Dueling Networks\n",
    "4. Exploration-exploitation trade-off\n",
    "5. Replay memory\n",
    "6. Target network and gradient descend step\n",
    "7. Double Q-Learning\n",
    "\n",
    "\n",
    "## 1. Reinforcement Learning\n",
    "In supervised learning (for instance) a neural network learns a function that maps an input to a corresponding output on the basis of a large amount of labeled training data consisting of example input-output pairs: Simply put, if you train a neural network to classify, for example, cats and dogs, you repeatedly show the network pictures of cats or dogs, compare the network's prediction to the label and slightly adapt the network's parameters until the neural net is able to classify what animal is shown in a picture.\n",
    "\n",
    "Now, let's say you let a child play a computer game it has never played before. In the case of [Breakout](https://www.youtube.com/watch?v=TmPfTpjtdgg) the player sees the pixel screen as input and has to decide whether to move left or right. You could certainly show the child many times in which situations it has to press left and in which situations right in order to win the game - this would be a classification problem (supervised learning) - but surely the child would become bored quickly and would try to push you aside, wanting to try the game itself. And the child would learn to play the game quickly without being told how to do so simply by evaluating which actions lead to an increased score. In reinfocement learning, we try to make a computer learn in this exact same way, by letting it explore the environment and occasionally giving it a reward when the score increases. \n",
    "\n",
    "However, in comparison to supervised learning, this poses a problem. On p. 1 of [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) (from now on denoted as (DQN)) the authors say:\n",
    "\n",
    ">RL algorithms [...] must be able to learn from a scalar reward signal that is frequently sparse [...] and delayed. The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.\n",
    "\n",
    "What do the authors mean with with \"sparse [...] and delayed\"? \n",
    "\n",
    "In our fictive maze example, the rewards are the sparser, the less gold you find. For an agent, a game is more difficult to learn, the sparser the reward is. [Pong](https://gym.openai.com/envs/Pong-v0/) is one of the games DQN can learn fastest because the score changes quite often. [Montezuma's Revenge](https://gym.openai.com/envs/MontezumaRevenge-v0/), on the other hand, has very sparse rewards and DQN (at least without some additional tricks) is not able to learn the game at all.\n",
    "\n",
    "And *delayed*?\n",
    "Imagine you walk through a maze trying to find treasures. You get a reward once you find gold. Now imagine you encounter a fork in the path. Which way do you take? As opposed to supervised learning, at the fork the agent does not get any reward for taking the right path but only later once it finds any gold. Yet it might have been crucial to take for example the left way at the fork. This is what the authors mean with *delayed*. The problem is met by discounting future rewards with a factor $\\gamma$ (between 0 and 1). \n",
    "\n",
    "The discounted return $R_i$ is calculated as follows:  $R_{i} = r_i + 𝛾 r_{i+1} + 𝛾^2 r_{i+2} + 𝛾^3 r_{i+3} + ...$\n",
    "\n",
    "Let us look at a very simple example where there is just one reward not equal to 0:\n",
    "\n",
    "time step | $t_{i}$ | $t_{i+1}$ |$t_{i+2}$ |$t_{i+3}$ |$t_{i+4}$ |\n",
    ":---| --- | ---| ---|---|---|\n",
    "reward | 0 | 0 | 0 | 1 | 0 |\n",
    "discounted reward | $\\gamma^3$ | $\\gamma^2$ | $\\gamma$ | 1 | 0 |\n",
    "for $gamma=0.9$|0.729 | 0.81 | 0.9|1 |0|\n",
    "\n",
    "Simply put, by discounting rewards, future rewards increase past or current rewards and the closer $\\gamma$ is to 1, the further they influence past rewards. \n",
    "\n",
    "## 2. Q-Learning\n",
    "So how does Q-Learning work? If the agent (regardless of trained or still untrained) is shown a state $s$ of the game, it has to decide which action $a$ to perform (for example move paddle left or right in breakout). How does it do that? On page 2 [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) define the so-called Q-Function:\n",
    ">We define the optimal action-value function $Q^∗(s, a)$ as the maximum expected return achievable by following any strategy, after seeing some sequence $s$ and then taking some action $a$ \n",
    "\n",
    "This means that given a state of the game $s$ (for now please consider *sequences* as states of the game), $Q^*(s,a)$ is the best (discounted) total score the agent can achieve if it performs action $a$ in the current state $s$. So how does it chose which action to perform assuming we already know $Q^*(s,a)$? One obvious strategy would be to always chose the action with the maximum value of $Q^*$ (we will see later, why this is slightly problematic). But first of all, we need to find this magical function $Q^*$:\n",
    "\n",
    "Let's say we are in state $s$, decide to perform action $a$ and arrive in the next state $s'$. If we assume that in state $s'$ the $Q^*$-values for all possible actions $a'$ were already known, then the $Q^*$-value in state $s$ for action $a$ (the maximum discounted return in $s$ for action $a$) would be the reward $r$ we got for performing action $a$ plus the discounted maximum future reward in $s'$:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^*(s,a) = r + \\gamma \\textrm{max} Q^*(s',a')\n",
    "\\end{equation}\n",
    "\n",
    "This is the so-called **Bellman equation**. Deep Q-Learning uses a neural network to find an approximation $Q(s,a,\\theta)$ of $Q^*(s,a)$. $\\theta$ are the parameters of the neural network. We will discuss later, how exactly the parameters of the network are updated. Now, I will explain to you, how the neural network maps a state $s$ to $Q$-values for the possible actions $a$.\n",
    "\n",
    "Earlier I mentioned, that I regard a *sequence* as a *state*. What did I mean with that? Imagine you have a pin-sharp image of a flying soccer ball. Can you tell in which direction it moves? No, you cannot (but you could if there was some kind of motion blur in the picture). The same problem occurs in Atari games. From a single frame of the game [Pong](https://gym.openai.com/envs/Pong-v0/), the agent can not discern in which direction the ball moves. DeepMind met this problem by stacking several consecutive frames and considering this sequence a state that is passed to the neural network. From such a sequence the agent is able to detect the direction and speed of movement because the ball is in a different position in each frame.\n",
    "\n",
    ">Since the agent only observes images of the current screen [...] it is impossible to fully understand the current situation from only the current screen $x_t$. We therefore consider sequences of actions and observations, $s_t = x_1, a_1, x_2, ..., a_{t−1}, x_t$, and learn game strategies that depend upon these sequences. All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state. ([page 2 of Mnih et al. 2013](https://arxiv.org/abs/1312.5602))\n",
    "\n",
    "You should defenitely understand what a [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process) and a [Markov chain\n",
    "](https://en.wikipedia.org/wiki/Markov_chain) is, they are fundamental to reinforcement learning. \n",
    "\n",
    "On page 5 of [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) the authors explain the preprocessing of the frames:\n",
    "\n",
    ">Working directly with raw Atari frames, which are 210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by first converting their RGB representation to gray-scale and down-sampling it to a 110×84 image. The final input representation is obtained by cropping an 84 × 84 region of the image that roughly captures the playing area. The final cropping stage is only required because we use the GPU implementation of 2D convolutions from [...], which expects square inputs. For the experiments in this paper, the function $\\phi$ [...] applies this preprocessing to the last 4 frames of a history and stacks them to produce the input to the $Q$-function.\n",
    "\n",
    "So let us start by looking at how the prepocessing can be implemented. I used `gym` from OpenAi to provide the environment. A frame returned by the environment has the shape `(210,160,3)` where the 3 stands for the RGB color channels. Such a frame is passed to the method `process` which transforms it to a `(84,84,1)` frame, where the 1 indicates that instead of three RGB channels there is one grayscale channel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiograetz/anaconda3/envs/DQN/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class processFrame():\n",
    "    def __init__(self):\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def process(self, sess, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return sess.run(self.processed, feed_dict={ self.frame:frame})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dueling Networks\n",
    "\n",
    "Instead of the network architecture described in [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) or [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) I used the dueling network architecture described in [Wang et al. 2016](https://arxiv.org/abs/1511.06581).\n",
    "\n",
    "![](pictures/dueling.png \"Figure 1 in Wang et al. 2016\")\n",
    "\n",
    "Both the [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) and the [Wang et al. 2016](https://arxiv.org/abs/1511.06581) dueling architecture have the same low-level convolutional structure:\n",
    "\n",
    ">The first convolutional layer has 32 8x8 filters with stride 4, the second 64 4x4 filters with stride 2, and the third and final convolutional layer consists 64 3x3 filters with stride 1.\n",
    "\n",
    "In the normal DQN architecture (top network in the figure) the *final hidden layer is fully-connected and consists of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.* (see page 6 of [Mnih et al. 2015](https://www.nature.com/articles/nature14236/)) These outputs are the predicted $Q(s,a;\\theta)$-values for action $a$ in state $s$.\n",
    "\n",
    "Instead of directly predicting a single $Q$-value for each action, the dueling architecture splits the final convolutional layer into two streams that represent the value and advantage functions that predict a *state value* $V(s)$ that depends only on the state, and *action advantages* $A(s,a)$ that depend on the state and the respective action. On page 2 of [Wang et al. 2016](https://arxiv.org/abs/1511.06581) the authors explain:\n",
    "\n",
    ">Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way. \n",
    "In the experiments, we demonstrate that the dueling architecture can more quickly identify the correct action during policy evaluation as redundant or similar actions are added to the learning problem. \n",
    "\n",
    "The *state value* $V(s)$ predicts *how good it is to be in a certain state* $s$ and the *action advantage* $A(s,a)$ predicts *relative measure of the importance of each action $a$ being in current state $s$*.\n",
    "I suggest you take a look at figure 2 in [Wang et al. 2016](https://arxiv.org/abs/1511.06581) to better understand what the value- and advantage-stream learn to look at.\n",
    "\n",
    "Next, we have to combine the value- and advantage-stream into $Q$-values $Q(s,a)$. This is done the following way (equation 9 in [Wang et al. 2016](https://arxiv.org/abs/1511.06581)):\n",
    "\n",
    "\\begin{equation}\n",
    "Q(s,a) = V(s) + \\left(A(s,a) - \\frac 1{| \\mathcal A |}\\sum_{a'}A(s, a')\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Why so complicated instead of just adding $V(s)$ and $A(s,a)$? Let's assume $Q(s,a) = V(s) + A(s,a)$. The Q function measures the value of choosing a particular action when in a particular state. The value function $V$, which is the expected value of $Q$ over all possible actions, measures how good it is to be in this particular state. If you combine $E(Q) = V$ and $Q = V + A$, you find $E(Q) = E(V) + E(A)$. But $V$ does not depend on any action, which means $E(V)=V$ and thus $E(A)=0$. The expected value of the advantage $A(s,a')$ over all possible actions $a'$ has to be zero. This is ensured be the equation shown above by subtracting the mean of the advantages of all actions from every advantage.\n",
    "\n",
    "In the cell below you find the code that implements this architecture in tensorflow. Some things to keep in mind: You should normalize the input pixel values to [0,1] by dividing the input with 255. The reason for this is, that the pixelvalues of the frames, the environment returns, are uint8 which can store values in the range [0,255]. Make sure you initialize the weights properly with the Xavier-initializer. DeepMind used an implementation of the RMSProp optimizer that is different to the one in tensorflow. Before implementing it myself, I tried the Adam optimizer which gave promising results without much hyperparameter-search. Adam was not invented when [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) was published, so one could argue that they might have used it instead of RMSProp if it had been invented earlier. On the other hand, the authors of this [blog post](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/) compare *Momentum, RMSProp and Adam* and argue:\n",
    ">Out of the above three, you may find momentum to be the most prevalent, despite Adam looking the most promising on paper. Empirical results have shown the all these algorithms can converge to different optimal local minima given the same loss. However, SGD with momentum seems to find more flatter minima than Adam, while adaptive methods tend to converge quickly towards sharper minima. Flatter minima generalize better than sharper ones.\n",
    "\n",
    "It might thus be well worth spending some time on playing with different optimizers and implementing the version of RMSProp used by DeepMind. For now, I stick with Adam and if I find some time in the future, I might come back to this.\n",
    "\n",
    "If you compare the dueling architecture described above to the network implemented in the next cell, you will find a small difference. Instead of two hidden fully connected layers with 512 rectifier units for each, the value- and the advantage-stream, I ended up adding a fourth convolutional layer with 512 filters that is then split into two streams. This architecture is suggested [here](https://github.com/awjuliani/DeepRL-Agents/blob/master/Double-Dueling-DQN.ipynb) and after performing some tests on the environment Pong, which is comparably easy to learn for a DQN agent, I find that this small adjustment lets the reward increase slightly earlier and higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, hidden=512, learningRate=0.00005):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden: Int; Number of filters in the final convolutional layer. This is different from the DeepMind implementation\n",
    "            learningRate: Float; learning rate for the Adam optimizer\n",
    "        \"\"\"\n",
    "        self.hidden = hidden\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None,84,84,4], dtype=tf.float32)\n",
    "        # Normalizing the input\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8,8], strides=4,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4,4], strides=2, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3,3], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv4 = tf.layers.conv2d(\n",
    "            inputs=self.conv3, filters=hidden, kernel_size=[7,7], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        \n",
    "        # Splitting into value- and advantage-stream\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4,2,3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream,units=env.action_space.n,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.value = tf.layers.dense(\n",
    "            inputs=self.valuestream,units=1,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # Combining value and advantage into Q-values as described above\n",
    "        self.Qvalues = self.value + tf.subtract(self.advantage,tf.reduce_mean(self.advantage,axis=1,keepdims=True))\n",
    "        self.bestAction = tf.argmax(self.Qvalues,1)\n",
    "        \n",
    "        ## The next lines perform the gradient descend step. You will understand this later.\n",
    "        \n",
    "        # targetQ according to Bellman equation Q = r + gamma*max Q', calculated in the function learn()\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        # Action that was performed\n",
    "        self.action = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        # Q value of the action that was performed\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qvalues, tf.one_hot(self.action, env.action_space.n, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        # Gradient descend step\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.targetQ, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learningRate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other interesting thing to notice: DeepMind uses the quadratic cost function with error clipping (see page 7 of [Mnih et al. 2015](https://www.nature.com/articles/nature14236/)).\n",
    "\n",
    ">We also found it helpful to clip the error term from the update [...] to be between -1 and 1. Because the absolute value loss function |x| has a derivative of -1 for all negative values of x and a derivative of 1 for all positive values of x, clipping the squared error to be between -1 and 1 corresponds to using an absolute value loss function for errors outside of the (-1,1) interval. This form of error clipping further improved the stability of the algorithm.\n",
    "\n",
    "Why does this improve the stability of the algorithm?\n",
    "\n",
    ">In deep networks or recurrent neural networks, error gradients can accumulate during an update and result in very large gradients. These in turn result in large updates to the network weights, and in turn, an unstable network. At an extreme, the values of weights can become so large as to overflow and result in NaN values. [Source](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)\n",
    "\n",
    "This so-called exploding gradient problem can, to some extent, be avoided by clippping the gradients to a certain threshold value, if they exceed it: * If the true gradient is larger than a critical value $x$, just assume it is $x$.* Observe that the derivate of the green curve does not increase (or decrease) for $x>1$ (or $x<-1$).\n",
    "\n",
    "![](pictures/huber.png \"See the gnuplot script in the pictures folder to find out how to quickly create this plot\")\n",
    "\n",
    "Error clipping can be easily implemented in tensorflow by using the Huber loss function `tf.losses.huber_loss`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploration-exploitation trade-off\n",
    "If you look at the code in the previous cell, you will find, that we are now able to predict the action, the network considers best (`self.bestAction`) by taking the argument of the maximum $Q$-value. But initially, the agent does not know how to play the game. If we always exploit and never explore by always chosing the action with the highest $Q$-value (greedy), the agent will stick to the first strategy it discovers that returns a small reward. It can then not continue exploring the environment and can not continue to learn. The $\\epsilon$-greedy algorithm offers a simple solution for that problem: Simply put, we usually chose the action the networks deems best but with a probability of $\\epsilon$ we chose a random action. $\\epsilon$ is a function of the number of frames the agent has seen. For the first 50000 frames the agent only explores ($\\epsilon=1$). Over the following 1 million frames, $\\epsilon$ is linearly decreased to 0.1, meaning that the agent starts exploiting more and more while it learns. DeepMind then keeps $\\epsilon=0.1$, however, I chose to decrease it to $\\epsilon=0.01$ over the remaining frames as suggested by the [OpenAi Baselines for DQN](https://blog.openai.com/openai-baselines-dqn/) (in the plot the maximum number of frames is 2 million for demonstrating purposes).\n",
    "\n",
    "![](pictures/epsilon.png \"See the gnuplot script in the pictures folder to find out how to quickly create this plot\")\n",
    "\n",
    "The method `getAction` in the cell below implements this behaviour: It first calculates $\\epsilon$ from the number of the current frame and then either returns a random action (with probability $\\epsilon$) or the action the DQN deems best. The variables in the constructor are the slopes and intercepts for the decrease of $\\epsilon$ shown in the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionGetter:\n",
    "    def __init__(self, explorationInitial = 1, explorationFinal = 0.1, explorationInference = 0.01, explorationAnnealingFrames = 1000000, memoryBufferStartSize = 50000, maxFrames = 25000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            explorationInitial: Float; Exploration probability for the first memoryBufferStartSize frames\n",
    "            explorationFinal: Float; Exploration probability after memoryBufferStartSize + explorationAnnealingFrames frames\n",
    "            explorationInference: Float; Exploration probability after maxFrames frames or during inference\n",
    "            explorationAnnealingFrames: Int: Number of frames over which the exploration probabilty is annealed from explorationInitial to explorationFinal\n",
    "            memoryBufferStartSize: Int; Number of frames during which the agent only explores\n",
    "            maxFrames: Int; Total number of learning frames\n",
    "        \"\"\"\n",
    "        self.explorationInitial = explorationInitial\n",
    "        self.explorationFinal = explorationFinal\n",
    "        self.explorationInference = explorationInference\n",
    "        self.explorationAnnealingFrames = explorationAnnealingFrames\n",
    "        self.memoryBufferStartSize = memoryBufferStartSize\n",
    "        self.maxFrames = maxFrames\n",
    "        \n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        self.m = -(self.explorationInitial - self.explorationFinal)/self.explorationAnnealingFrames\n",
    "        self.b = self.explorationInitial - self.m*self.memoryBufferStartSize\n",
    "        self.m2 = -(self.explorationFinal - self.explorationInference)/(self.maxFrames - self.explorationAnnealingFrames - self.memoryBufferStartSize)\n",
    "        self.b2 = self.explorationInference - self.m2*self.maxFrames\n",
    "\n",
    "    def getAction(self, frameNumber, state, inference=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frameNumber: An integer determining the number of the current frame\n",
    "            state: A (84, 84, 4) sequence of frames of an Atari game in grayscale\n",
    "            inference: A boolean saying whether the agent is learning (inference=False)\n",
    "        Returns:\n",
    "            An integer between 0 and env.action_space.n - 1 determining the action the agent perfoms next\n",
    "        \"\"\"\n",
    "        if frameNumber < self.memoryBufferStartSize:\n",
    "            e = self.explorationInitial\n",
    "        elif frameNumber >= self.memoryBufferStartSize and frameNumber < self.memoryBufferStartSize + self.explorationAnnealingFrames:\n",
    "            e = self.m*frameNumber + self.b\n",
    "        elif frameNumber >= self.memoryBufferStartSize + self.explorationAnnealingFrames:\n",
    "            e = self.m2*frameNumber + self.b2\n",
    "        elif inference:\n",
    "            e = self.explorationInference\n",
    "        if np.random.rand(1) < e:\n",
    "            return np.random.randint(0, env.action_space.n)\n",
    "        else:\n",
    "            return sess.run(mainDQN.bestAction, feed_dict={mainDQN.input:[state]})[0]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how the DQN predicts the best action and we have a simple answer to the exploration exploitation dilemma.\n",
    "So, what else do we need to make it work? At this point I suggest, we take a look at the algorithm presented on page 7 in [Mnih et al. 2015](https://www.nature.com/articles/nature14236/).\n",
    "\n",
    "![](pictures/DQN.png)\n",
    "\n",
    "Let us go through the algorithm step by step:\n",
    "* We do not know yet what *replay memory* D is.\n",
    "* Action-value function Q is our DQN network, that we already implemented.\n",
    "* We need to discuss why a second Q network called *target* action-value function is needed.\n",
    "* At the beginning of each episode a sequence is initalized. This is implemented by stacking four (grayscale) frames together as discussed above.\n",
    "* We discussed how the action is selected ($\\epsilon$-greedy).\n",
    "* When the actions is performed, the environment returns the next frame and the reward for that action. `Gym` additionaly returns a boolean I call `terminal` that states whether the game is over and a dictionary containing the number of lives the agent has left (`ale.lives`). \n",
    "* We do not know yet, what it means to store a transition in the replay memory D. A list `[state, action, reward, terminal, nextState]` is called transition. A `state` are four frames stacked together. `newState` is produced by stacking the observed frame (after the action is performed) onto `state` and removing the oldest frame. You will see the implementation later. \n",
    "* We have to discuss, how a minibatch is retured from the replay memory and how the gradient descend step is performed.\n",
    "* Finally we have to look at why and how the target Q network is reset to the main Q netowork.\n",
    "\n",
    "Let's continue with the replay memory\n",
    "\n",
    "## 5. Replay memory\n",
    "\n",
    ">Second, learning directly from consecutive samples is inefficient, due to the strong correlations between the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates. Third, when learning on-policy the current parameters determine the next data sample that the parameters are trained on. For example, if the maximizing action is to move left then the training samples will be dominated by samples from the left-hand side; if the maximizing action then switches to the right then the training distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically. ([page 5 of Mnih et al. 2013](https://arxiv.org/abs/1312.5602))\n",
    "\n",
    "This means that when we chose an action and perform a step to receive a reward, the network does not learn from this last step but rather adds the transition to the replay memory. It then draws a random minibatch from the replay memory to perform a gradient descent step.\n",
    "\n",
    "The replay memory stores the last one million transitions. Let's recall that a transition is `[state, action, reward, terminal, nextState]`. We therefore need to store the last 1 million `state`, `action`, `reward`, `terminal` and `newState` observed after the `action` was performed. If you remember that `state` and `newState` are four frames each, that would be eight million frames. However, since `newState` is created by stacking the newest frame on top of `state` and deleting the oldest frame, `newState` and `state` share three frames. Furthermore, `newState` of transition i will be `state` of transition i+1. This means that it is sufficient to store the last one million frames in the replay memory and then stacking four frames toghether when we need a `state` and `newState`. \n",
    "\n",
    "With one million frames of 84 by 84 pixels that need to be stored in your computers memory, we need to consider in what datatype we store them. The environment returns frames with pixel values stored as `uint8` which can have values ranging from 0 to 255. A `uint8` needs 8 bits. The network expects a `tf.float32` input with pixel values between 0 and 1 (which takes four times more space than a `uint8`). Since we want to reduce the memory requirements, we store the frames in `uint8` and divide them by 255 before passing them to the network.\n",
    "\n",
    "When implementing my version of replay memory, I looked at this [code](https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py) and ended up implementing my replay memory with some adjustments that make the code more understandable in my opinion.\n",
    "\n",
    "Let's look at the `ReplayMemory` class below. In the constructor, we pre-allocate the memory for the the actions, the rewards, the frames, the terminal flags and also for the states and new states of the minibatch. \n",
    "\n",
    "In the `addExperience` method the frames etc. are written into `self.frames` at index `self.current` which is then increased by 1. When `self.current` reaches the size of the replay memory (1 million), it is reset to zero to overwrite the oldest frames. The method `_getState` slices four frames out of `self.frames` and returns them as a `state`.\n",
    "\n",
    "To understand what the method `_getValidIndices` does, we need to understand what a invalid index is. We store all frames the agent sees in `self.frames`. When a game terminates (`terminal=True`) at index i, frame at index i belongs to a different episode than the frame at i+1. We want to avoid creating a `state` from two different episodes. The same thing can happen at the index `self.current`. Finally we need to make sure that an index is not smaller than the number of frames stacked toghether to create a `state` (`self.agentHistoryLength=4`), so that a `state` and `newState` can be sliced our of the array. The method `_getValidIndices` finds 32 (size of minibatch) such indices.\n",
    "The method `getMinibatch` draws returns the transitions for those indices. Pay attention that we need to transpose `self.states` and `self.newStates` before returning them: the DQN expects an input of the dimension `[None,84,84,4]` whereas `_getState` returns a `state` of the dimension `[4,84,84]`\n",
    "\n",
    "We now know 1) why a replay memory greatly improves the stability of the algorithm, 2) how store a transition in the replay memory and 3) how a minibatch is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, size = 1000000, frameHeight=84, frameWidth=84, agentHistoryLength = 4, batchSize = 32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: Integer; Number of stored transitions\n",
    "            frameHeight: Integer; Height of a frame of an Atari game\n",
    "            frameWidth: Integer: Width of a frame of an Atari game\n",
    "            agentHistoryLength: Integer: Number of frames stacked together to create a state\n",
    "            batchSize: Integer; Number if transitions returned in a minibatch\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frameHeight = frameHeight\n",
    "        self.frameWidth = frameWidth\n",
    "        self.agentHistoryLength = agentHistoryLength\n",
    "        self.batchSize = batchSize\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frameHeight,self.frameWidth), dtype=np.uint8)\n",
    "        self.terminalFlags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the States and newStates in a minibatch\n",
    "        self.states = np.empty((self.batchSize, self.agentHistoryLength, self.frameHeight, self.frameWidth), dtype=np.uint8)\n",
    "        self.newStates = np.empty((self.batchSize, self.agentHistoryLength, self.frameHeight, self.frameWidth), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batchSize, dtype=np.int32)\n",
    "        \n",
    "    def addExperience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: An integer between 0 and env.action_space.n - 1 determining the action the agent perfoms next\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frameHeight, self.frameWidth):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current,...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminalFlags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _getState(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agentHistoryLength - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agentHistoryLength+1:index+1,...]\n",
    "        \n",
    "    def _getValidIndices(self):\n",
    "        for i in range(self.batchSize):\n",
    "            while True:\n",
    "                index = random.randint(self.agentHistoryLength, self.count - 1)\n",
    "                if index < self.agentHistoryLength:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agentHistoryLength <= self.current:\n",
    "                    continue\n",
    "                if self.terminalFlags[index - self.agentHistoryLength:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def getMinibatch(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        if self.count < self.agentHistoryLength:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._getValidIndices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._getState(idx - 1)\n",
    "            self.newStates[i] = self._getState(idx)\n",
    "        \n",
    "        return np.transpose(self.states,axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.newStates,axes=(0,2,3,1)), self.terminalFlags[self.indices]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target network and gradient descend step\n",
    "\n",
    "Why do we need two networks, the action-value function and the *target* action-value function?\n",
    "\n",
    "We perform a gradient descent step, to regress the current $Q_\\text{prediction}$-value towards the $Q_\\text{target}$-value given by the Bellmann equation, that I introduced above. In order to do that, we have to derive the loss function. Let's consider the quadratic loss function instead of the Huber loss function for simplicity:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac 12\\left(Q_\\text{prediction} - Q_\\text{target}\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "$Q_\\text{prediction}$ is calculated in the `DQN` class (`self.Qvalues`). $Q_\\text{prediction}$ depends on the parameters $\\theta$ of the network that estimates it.\n",
    "\n",
    "The $Q_\\text{target}$ value is calculated according to the Bellmann equation. It is the sum of the immediate reward and the discounted estimated future reward from performing the best action in the next state:\n",
    "\n",
    "Target:\n",
    "\\begin{equation}\n",
    "Q_\\text{target}(s,a) = r + \\gamma \\textrm{max} Q(s',a')\n",
    "\\end{equation}\n",
    "\n",
    "This is not done in the `DQN` class but in the `learn` method below. The calculated value is then passed to the placeholder called `self.targetQ` in the `DQN` class. There, the gradient descent step is performed.\n",
    "\n",
    "So, now that we understand how the gradient descend step works, why use two networks?\n",
    "\n",
    "The problem is that both $Q_\\text{prediction}$ and $Q_\\text{target}$ depend on the same parameters $\\theta$ if only one network is used. This can lead to instability when regressing $Q_\\text{prediction}$ towards $Q_\\text{target}$.\n",
    "\n",
    "On page 1 of [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) the authors explain:\n",
    ">Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function. This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values [...] and the target values [...].\n",
    "We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution [...]. Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target. \n",
    "\n",
    "Therefore they used one network to predict the $Q_\\text{prediction}$-value and the other fixed network to predict the Q_\\text{target}-value. The main network is optimized during the gradient descend step and every 10000 steps the main networks parameters are copied to the target network.\n",
    "\n",
    "However, there is one additional very powerfull improvement called *Double Q-Learning*.\n",
    "\n",
    "## 7. Double Q-Learning\n",
    "DQN has been observed to estimate unrealistically high $Q$-values. The reason for this is, that the Bellmann equation *includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values* (see [van Hasselt et al. 2016, page 1](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)). \n",
    "\n",
    "The authors furthermore explain:\n",
    "\n",
    ">If all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse. [...]\n",
    "If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. [...]\n",
    "We then show that this algorithm not only yields more accurate value estimates, but leads to much higher scores on several games. This demonstrates that the overestimations of DQN were indeed leading to poorer policies and that it is beneficial to reduce them \n",
    "\n",
    "The estimated $Q$-values are noisy. Assume that the true $Q$-value is 0 for all actions. But because of the noisy estimation, some $Q$-values might be slightly positive, others slightly negative. The max operation in the Bellmann equation will however always chose the small positive values, despite the fact, that those actions are not truly better. The estimatation of $Q$-values is thus biased towards larger values. How do we fix this?\n",
    "Instead of estimating the $Q$-values in the next state $Q(s',a')$ with only the target network, we use the main network to estimate which action is the best and then ask the target network how high the $Q$-value is for that action. This way, the main network will still prefer the action with the small positive $Q$-value but because of the noise estimation, the target network will predict a small positive **or** small negative $Q$-value for that action and on average, the predicted $Q$-value will be closer to 0.\n",
    "\n",
    "Mathematically, the reason for the overestimation is, that the expectation of a maximum is greater than or equal to the maximum of an expectation [van Hasselt 2013, Theorem 1](https://arxiv.org/abs/1302.7175).\n",
    "\n",
    "The new Bellmann equation changes from\n",
    "\n",
    "\\begin{equation}\n",
    "Q_\\text{target}(s,a) = r + \\gamma \\textrm{max} Q(s',a';\\theta_\\text{target})\n",
    "\\end{equation}\n",
    "\n",
    "to\n",
    "\n",
    "\\begin{equation}\n",
    "Q_\\text{target}(s,a) = r + \\gamma Q\\left(s',a'=\\text{argmax} Q(s',a';\\theta_\\text{main});\\theta_\\text{target}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "I know that this equation might look discouraging to people who do not have some kind of math background. So let's describe it again in words:\n",
    "\n",
    "The main network estimates which action $a'$ (in the next state $s'$) is best (thats the $\\text{argmax} Q(s',a';\\theta_\\text{main})$ part). The target network then estimates what the $Q$-value for that action is. This $Q$-value has to be discounted with $\\gamma$ and is then added to the reward the agent got for action $a$ (not $a'$ in the next state!).\n",
    "\n",
    "One more thing:\n",
    "If the game is over (`terminal=True`) because the agend lost or won, there is no next state and the $Q_\\text{target}$-value is simply the reward $r$.\n",
    "\n",
    "Look at the implementation in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn():\n",
    "    \"\"\"\n",
    "        Draws a minibatch from the replay memory, calculates the target Q-value the current Q-value \n",
    "        is regressed to. Then a gradient descend step is performed.\n",
    "    \"\"\"\n",
    "    # Draw a minibatch from the replay memory\n",
    "    states, actions, rewards, newStates, terminalFlags = myMemoryBuffer.getMinibatch()    \n",
    "    # The main network estimates which action is best (in the next state s', newStates is passed!) for every transition in the minibatch\n",
    "    argQmax = sess.run(mainDQN.bestAction, feed_dict={mainDQN.input:newStates})\n",
    "    # The target network estimates the Q-values (in the next state s', newStates is passed!) for every transition in the minibatch\n",
    "    Qvals = sess.run(targetDQN.Qvalues, feed_dict={targetDQN.input:newStates})\n",
    "    doubleQ = Qvals[range(bs), argQmax]\n",
    "    # Bellman equation. Multiplication with (1-terminalFlags) makes sure that if the game is over, targetQ=rewards\n",
    "    targetQ = rewards + (discountFactor*doubleQ * (1-terminalFlags))\n",
    "    # Gradient descend step to update the parameters of the main network\n",
    "    _ = sess.run(mainDQN.update,feed_dict={mainDQN.input:states,mainDQN.targetQ:targetQ, mainDQN.action:actions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I already mentioned, the parameters of the main network are periodically copied every 10000 steps to the target network. This is implemented in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater:\n",
    "    def __init__(self, mainDQNVars, targetDQNVars):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mainDQNVars: A list of tensorflow variables belonging to the main DQN network\n",
    "            targetDQNVars: A list of tensorflow variables belonging to the target DQN network\n",
    "        \"\"\"\n",
    "        self.mainDQNVars = mainDQNVars\n",
    "        self.targetDQNVars = targetDQNVars\n",
    "\n",
    "    def _updateTargetVars(self):\n",
    "        updateOps = []\n",
    "        for i, var in enumerate(self.mainDQNVars):\n",
    "            op = self.targetDQNVars[i].assign(var.value())\n",
    "            updateOps.append(op)\n",
    "        return updateOps\n",
    "            \n",
    "    def updateNetworks(self, sess):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "        Assigns the values of the parameters of the main network to the parameters of the target network\n",
    "        \"\"\"\n",
    "        updateOps = self._updateTargetVars()\n",
    "        for op in updateOps:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the cell below creates a gif from a sequence of frames passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateGif(sess, frameNumber, framesForGif, reward):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frameNumber: An integer determining the number of the current frame\n",
    "            framesForGif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
    "            reward: Integer. Total reward of the episode that es ouputted as a gif\n",
    "    \"\"\"\n",
    "    for idx,frame_idx in enumerate(framesForGif): \n",
    "        framesForGif[idx] = resize(frame_idx,(420,320,3),preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{PATH}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frameNumber, reward)}', framesForGif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has 4 possible actions ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "# Control parameter\n",
    "maxEpisodeLength = 18000\n",
    "\n",
    "targetNetworkUpdateFreq = 10000\n",
    "discountFactor = 0.99\n",
    "memoryBufferStartSize = 50000\n",
    "maxFrames = 25000000\n",
    "memorySize = 1000000\n",
    "noOpSteps = 20\n",
    "gifFreq = 50\n",
    "\n",
    "hidden = 512\n",
    "learningRate = 0.00001\n",
    "bs = 32\n",
    "\n",
    "PATH = \"output/\"\n",
    "os.makedirs(PATH,exist_ok=True)\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v0')\n",
    "print(\"The environment has {} possible actions {}\".format(env.action_space.n, env.unwrapped.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to make sure, that the environment ...\n",
    "env = gym.make('BreakoutDeterministic-v0').env.ale\n",
    "env.getFloat('repeat_action_probability')\n",
    "should be 0.25 like in ALE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fabiograetz/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiograetz/anaconda3/envs/DQN/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/fabiograetz/anaconda3/envs/DQN/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 188 1.0 187\n",
      "10 2049 1.2727272727272727 136\n",
      "20 3897 1.2857142857142858 251\n",
      "30 5598 1.1612903225806452 140\n",
      "40 7362 1.146341463414634 231\n",
      "50 9185 1.1372549019607843 225\n",
      "60 10877 1.0819672131147542 227\n",
      "70 12716 1.1126760563380282 171\n",
      "80 14616 1.1481481481481481 176\n",
      "90 16575 1.1978021978021978 165\n",
      "100 18582 1.26 151\n",
      "110 20453 1.28 213\n",
      "120 22173 1.24 154\n",
      "130 24253 1.34 237\n",
      "140 26191 1.36 338\n",
      "150 28176 1.41 160\n",
      "160 30142 1.48 221\n",
      "170 31817 1.44 160\n",
      "180 33924 1.5 171\n",
      "190 35652 1.44 207\n",
      "200 37566 1.39 280\n",
      "210 39265 1.34 153\n",
      "220 41205 1.4 206\n",
      "230 42924 1.32 241\n",
      "240 44775 1.32 186\n",
      "250 46938 1.42 318\n",
      "260 48667 1.37 148\n",
      "270 50548 1.42 395\n",
      "280 52307 1.32 257\n",
      "290 54329 1.39 146\n",
      "300 56291 1.41 238\n",
      "310 58420 1.53 209\n",
      "320 60135 1.46 164\n",
      "330 62174 1.5 249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cd3d69b93aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframeNumber\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmemoryBufferStartSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframeNumber\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtargetNetworkUpdateFreq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframeNumber\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmemoryBufferStartSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f73e3bc59853>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewStates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminalFlags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyMemoryBuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMinibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0margQmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestAction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnewStates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mQvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnewStates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DQN/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "myMemoryBuffer = MemoryBuffer(size=memorySize, batchSize=bs)\n",
    "mainDQN = DQN(hidden, learningRate)\n",
    "targetDQN = DQN(hidden)\n",
    "variables = tf.trainable_variables()\n",
    "mainDQNVars = variables[0:len(variables)//2]\n",
    "targetDQNVars = variables[len(variables)//2:]\n",
    "\n",
    "NetworkUpdater = TargetNetworkUpdater(mainDQNVars, targetDQNVars)\n",
    "frameProcessor = processFrame()\n",
    "actionGetter = ActionGetter(memoryBufferStartSize=memoryBufferStartSize, maxFrames=maxFrames)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "restore = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if restore == True:\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "\n",
    "    frameNumber = 0\n",
    "    episodeNumber=0\n",
    "    rewards = []\n",
    "    \n",
    "    while frameNumber < maxFrames:\n",
    "        if episodeNumber % gifFreq == 0: \n",
    "            framesForGif = []\n",
    "        \n",
    "        frame = env.reset()\n",
    "        terminal = False\n",
    "        terminal2 = False\n",
    "        lastLives = 0\n",
    "        \n",
    "        # No op steps\n",
    "        for _ in range(random.randint(1, noOpSteps)):\n",
    "            frame, _, _, _ = env.step(0)\n",
    "            \n",
    "        processedFrame = frameProcessor.process(sess,frame)\n",
    "        state = np.repeat(processedFrame,4, axis=2)\n",
    "        episodeRewardSum = 0\n",
    "        \n",
    "        for j in range(maxEpisodeLength):\n",
    "            action = actionGetter.getAction(frameNumber,state)\n",
    "            newFrame, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            # Pass terminal=True to Memory if live was lost\n",
    "            if info['ale.lives'] < lastLives:\n",
    "                terminal2 = True;\n",
    "            else:\n",
    "                terminal2 = terminal\n",
    "            lastLives = info['ale.lives']\n",
    "            \n",
    "            if episodeNumber % gifFreq == 0: \n",
    "                framesForGif.append(newFrame)\n",
    "                        \n",
    "            processedNewFrame = frameProcessor.process(sess,newFrame)\n",
    "            newState = np.append(state[:,:,1:],processedNewFrame,axis=2)\n",
    "\n",
    "            frameNumber += 1\n",
    "            \n",
    "            # Add current experience to Memory\n",
    "            myMemoryBuffer.addExperience(action=action, frame=processedNewFrame[:,:,0], reward=reward, terminal=terminal2)\n",
    "            \n",
    "            if frameNumber > memoryBufferStartSize:\n",
    "                learn()\n",
    "            \n",
    "            if frameNumber % targetNetworkUpdateFreq == 0 and frameNumber > memoryBufferStartSize:\n",
    "                NetworkUpdater.updateNetworks(sess)\n",
    "            \n",
    "            episodeRewardSum += reward\n",
    "            state = newState\n",
    "            \n",
    "            if terminal == True:\n",
    "                break\n",
    "                \n",
    "        rewards.append(episodeRewardSum)\n",
    "        if episodeNumber % gifFreq == 0: \n",
    "            generateGif(sess, frameNumber, framesForGif, episodeRewardSum)\n",
    "        if episodeNumber % gifFreq == 0:\n",
    "            saver.save(sess,PATH+'/my_model',global_step=frameNumber)\n",
    "        if episodeNumber % 10 == 0:\n",
    "            print(episodeNumber, frameNumber,np.mean(rewards[-100:]), j)\n",
    "            with open('rewards.dat','a') as f:\n",
    "                print(episodeNumber, frameNumber,np.mean(rewards[-100:]), j,file=f)\n",
    "        episodeNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()\n",
    "frameProcessor = processFrame()\n",
    "mainDQN = DQN(hidden, learningRate)\n",
    "targetDQN = DQN(hidden)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "    framesForGif = []\n",
    "    terminal = False\n",
    "    frame = env.reset()\n",
    "    processedFrame = frameProcessor.process(sess,frame)\n",
    "    state = np.repeat(processedFrame,4, axis=2)\n",
    "    episodeRewardSum = 0\n",
    "    \n",
    "    while not terminal:\n",
    "        action = getAction(1,state,inference=True)\n",
    "        newFrame, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "        framesForGif.append(newFrame)\n",
    "                        \n",
    "        processedNewFrame = frameProcessor.process(sess,newFrame)\n",
    "        newState = np.append(state[:,:,1:],processedNewFrame,axis=2)\n",
    "\n",
    "\n",
    "        episodeRewardSum += reward\n",
    "        state = newState\n",
    "    print(\"Total reward: %s\" % episodeRewardSum)\n",
    "    generateGif(sess,0, framesForGif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
