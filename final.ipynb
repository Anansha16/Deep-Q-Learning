{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the introduction to this notebook first...\n",
    "\n",
    "... or dive right in :)\n",
    "\n",
    "## Contents\n",
    "1. Reinforcement Learning \n",
    "\n",
    "\n",
    "## 1. Reinforcement Learning\n",
    "In supervised learning (for instance) a neural network learns a function that maps an input to a corresponding output on the basis of a large amount of labeled training data consisting of example input-output pairs: Simply put, if you train a neural network to classify, for example, cats and dogs, you repeatedly show the network pictures of cats or dogs, compare the network's prediction to the label and slightly adapt the network's parameters until the neural net is able to classify what animal is shown in a picture.\n",
    "\n",
    "Now, let's say you let a child play a computer game it has never played before. In the case of [Breakout](https://www.youtube.com/watch?v=TmPfTpjtdgg) the player sees the pixel screen as input and has to decide whether to move left or right. You could certainly show the child many times in which situations it has to press left and in which situations right in order to win the game - this would be a classification problem (supervised learning) - but surely the child would become bored quickly and would try to push you aside, wanting to try the game itself. And the child would learn to play the game quickly without being told how to do so simply by evaluating which actions lead to an increased score. In reinfocement learning, we try to make a computer learn in this exact same way, by letting it explore the environment and occasionally giving it a reward when the score increases. \n",
    "\n",
    "However, in comparison to supervised learning, this poses a problem. On p. 1 of [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) (from now on denoted as (DQN)) the authors say:\n",
    "\n",
    ">RL algorithms [...] must be able to learn from a scalar reward signal that is frequently sparse [...] and delayed. The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.\n",
    "\n",
    "What do the authors mean with with \"sparse [...] and delayed\"? \n",
    "\n",
    "In our fictive maze example, the rewards are the sparser, the less gold you find. For an agent, a game is more difficult to learn, the sparser the reward is. [Pong](https://gym.openai.com/envs/Pong-v0/) is one of the games DQN can learn fastest because the score changes quite often. [Montezuma's Revenge](https://gym.openai.com/envs/MontezumaRevenge-v0/), on the other hand, has very sparse rewards and DQN (at least without some additional tricks) is not able to learn the game at all.\n",
    "\n",
    "And *delayed*?\n",
    "Imagine you walk through a maze trying to find treasures. You get a reward once you find gold. Now imagine you encounter a fork in the path. Which way do you take? As opposed to supervised learning, at the fork the agent does not get any reward for taking the right path but only later once it finds any gold. Yet it might have been crucial to take for example the left way at the fork. This is what the authors mean with *delayed*. The problem is met by discounting future rewards with a factor $\\gamma$ (between 0 and 1). \n",
    "\n",
    "The discounted return $R_i$ is calculated as follows:  $R_{i} = r_i + 𝛾 r_{i+1} + 𝛾^2 r_{i+2} + 𝛾^3 r_{i+3} + ...$\n",
    "\n",
    "Let us look at a very simple example where there is just one reward not equal to 0:\n",
    "\n",
    "time step | $t_{i}$ | $t_{i+1}$ |$t_{i+2}$ |$t_{i+3}$ |$t_{i+4}$ |\n",
    ":---| --- | ---| ---|---|---|\n",
    "reward | 0 | 0 | 0 | 1 | 0 |\n",
    "discounted reward | $\\gamma^3$ | $\\gamma^2$ | $\\gamma$ | 1 | 0 |\n",
    "for $gamma=0.9$|0.729 | 0.81 | 0.9|1 |0|\n",
    "\n",
    "Simply put, by discounting rewards, future rewards increase past or current rewards and the closer $\\gamma$ is to 1, the further they influence past rewards. \n",
    "\n",
    "## 2. Q-Learning\n",
    "So how does Q-Learning work? If the agent (regardless of trained or still untrained) is shown a state $s$ of the game, it has to decide which action $a$ to perform (for example move paddle left or right in breakout). How does it do that? On page 2 [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) define the so-called Q-Function:\n",
    ">We define the optimal action-value function $Q^∗(s, a)$ as the maximum expected return achievable by following any strategy, after seeing some sequence $s$ and then taking some action $a$ \n",
    "\n",
    "This means that given a state of the game $s$ (for now please consider *sequences* as states of the game), $Q^*(s,a)$ is the best (discounted) total score the agent can achieve if it performs action $a$ in the current state $s$. So how does it chose which action to perform assuming we already know $Q^*(s,a)$? One obvious strategy would be to always chose the action with the maximum value of $Q^*$ (we will see later, why this is slightly problematic). But first of all, we need to find this magical function $Q^*$:\n",
    "\n",
    "Let's say we are in state $s$, decide to perform action $a$ and arrive in the next state $s'$. If we assume that in state $s'$ the $Q^*$-values for all possible actions $a'$ were already known, then the $Q^*$-value in state $s$ for action $a$ (the maximum discounted return in $s$ for action $a$) would be the reward $r$ we got for performing action $a$ plus the discounted maximum future reward in $s'$:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^*(s,a) = r + \\gamma \\textrm{max} Q^*(s',a')\n",
    "\\end{equation}\n",
    "\n",
    "This is the so-called **Bellman equation**. Deep Q-Learning uses a neural network to find an approximation $Q(s,a,\\theta)$ of $Q^*(s,a)$. $\\theta$ are the parameters of the neural network. We will discuss later, how exactly the parameters of the network are updated. Now, I will explain to you, how the neural network maps a state $s$ to $Q$-values for the possible actions $a$.\n",
    "\n",
    "Earlier I mentioned, that I regard a *sequence* as a *state*. What did I mean with that? Imagine you have a pin-sharp image of a flying soccer ball. Can you tell in which direction it moves? No, you cannot (but you could if there was some kind of motion blur in the picture). The same problem occurs in Atari games. From a single frame of the game [Pong](https://gym.openai.com/envs/Pong-v0/), the agent can not discern in which direction the ball moves. DeepMind met this problem by stacking several consecutive frames and considering this sequence a state that is passed to the neural network. From such a sequence the agent is able to detect the direction and speed of movement because the ball is in a different position in each frame.\n",
    "\n",
    "On page 5 of [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) the authors explain the preprocessing of the frames:\n",
    "\n",
    ">Working directly with raw Atari frames, which are 210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by first converting their RGB representation to gray-scale and down-sampling it to a 110×84 image. The final input representation is obtained by cropping an 84 × 84 region of the image that roughly captures the playing area. The final cropping stage is only required because we use the GPU implementation of 2D convolutions from [...], which expects square inputs. For the experiments in this paper, the function $\\phi$ [...] applies this preprocessing to the last 4 frames of a history and stacks them to produce the input to the $Q$-function.\n",
    "\n",
    "So let us start by looking at how the prepocessing can be implemented. I used `gym` from OpenAi to provide the environment. A frame returned by the environment has the shape `(210,160,3)` where the 3 stands for the RGB color channels. Such a frame is passed to the method `process` which transforms it to a `(84,84,1)` frame, where the 1 indicates that instead of three RGB channels there is one grayscale channel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class processFrame():\n",
    "    def __init__(self):\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def process(self, sess, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return sess.run(self.processed, feed_dict={ self.frame:frame})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On page 6 the authors explain the architecture of the network:\n",
    ">The input to the neural network consists is an 84 × 84 × 4 image [...]. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully- connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered. We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN).\n",
    "\n",
    "next, do the dueling architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, hidden=512, learningRate=0.00005):\n",
    "        self.hidden = hidden\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None,84,84,4], dtype=tf.float32)\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8,8], strides=4,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4,4], strides=2, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3,3], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv4 = tf.layers.conv2d(\n",
    "            inputs=self.conv3, filters=hidden, kernel_size=[7,7], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        \n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4,2,3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream,units=env.action_space.n,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.value = tf.layers.dense(\n",
    "            inputs=self.valuestream,units=1,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.Qvalues = self.value + tf.subtract(self.advantage,tf.reduce_mean(self.advantage,axis=1,keepdims=True))\n",
    "        self.bestAction = tf.argmax(self.Qvalues,1)\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qvalues, tf.one_hot(self.action, env.action_space.n, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.targetQ, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learningRate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater:\n",
    "    def __init__(self, mainDQNVars, targetDQNVars):\n",
    "        self.mainDQNVars = mainDQNVars\n",
    "        self.targetDQNVars = targetDQNVars\n",
    "\n",
    "    def _updateTargetVars(self):\n",
    "        updateOps = []\n",
    "        for i, var in enumerate(self.mainDQNVars):\n",
    "            op = self.targetDQNVars[i].assign(var.value())\n",
    "            updateOps.append(op)\n",
    "        return updateOps\n",
    "            \n",
    "    def updateNetworks(self, sess):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        updateOps = self._updateTargetVars()\n",
    "        for op in updateOps:\n",
    "            sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, size = 1000000, frameHeight=84, frameWidth=84, agentHistoryLength = 4, batchSize = 32):\n",
    "        self.size = size\n",
    "        self.frameHeight = frameHeight\n",
    "        self.frameWidth = frameWidth\n",
    "        self.agentHistoryLength = agentHistoryLength\n",
    "        self.batchSize = batchSize\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frameHeight,self.frameWidth), dtype=np.uint8)\n",
    "        self.terminalFlags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the States and newStates in a minibatch\n",
    "        self.states = np.empty((self.batchSize, self.agentHistoryLength, self.frameHeight, self.frameWidth), dtype=np.uint8)\n",
    "        self.newStates = np.empty((self.batchSize, self.agentHistoryLength, self.frameHeight, self.frameWidth), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batchSize, dtype=np.int32)\n",
    "        \n",
    "    def addExperience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frameHeight, self.frameWidth):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current,...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminalFlags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _getState(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agentHistoryLength - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agentHistoryLength+1:index+1,...]\n",
    "        \n",
    "    def _getValidIndices(self):\n",
    "        for i in range(self.batchSize):\n",
    "            while True:\n",
    "                index = random.randint(self.agentHistoryLength, self.count - 1)\n",
    "                if index < self.agentHistoryLength:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agentHistoryLength <= self.current:\n",
    "                    continue\n",
    "                if self.terminalFlags[index - self.agentHistoryLength:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def getMinibatch(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        if self.count < self.agentHistoryLength:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._getValidIndices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._getState(idx - 1)\n",
    "            self.newStates[i] = self._getState(idx)\n",
    "        \n",
    "        return np.transpose(self.states,axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.newStates,axes=(0,2,3,1)), self.terminalFlags[self.indices]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionGetter:\n",
    "    def __init__(self, explorationInitial = 1, explorationFinal = 0.1, explorationInference = 0.01, explorationDecayFrames = 1000000, memoryBufferStartSize = 5000, maxFrames = 50000000):\n",
    "        self.explorationInitial = explorationInitial\n",
    "        self.explorationFinal = explorationFinal\n",
    "        self.explorationInference = explorationInference\n",
    "        self.explorationDecayFrames = explorationDecayFrames\n",
    "        self.memoryBufferStartSize = memoryBufferStartSize\n",
    "        self.maxFrames = maxFrames\n",
    "        \n",
    "        # Slopes and intercepts for exploration decay\n",
    "        self.m = -(self.explorationInitial - self.explorationFinal)/self.explorationDecayFrames\n",
    "        self.b = self.explorationInitial - self.m*self.memoryBufferStartSize\n",
    "        self.m2 = -(self.explorationFinal - self.explorationInference)/(self.maxFrames - self.explorationDecayFrames - self.memoryBufferStartSize)\n",
    "        self.b2 = self.explorationFinal - self.m2*(self.memoryBufferStartSize + self.explorationDecayFrames)\n",
    "\n",
    "    def getAction(self, frameNumber, state, inference=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        if frameNumber < self.memoryBufferStartSize:\n",
    "            e = self.explorationInitial\n",
    "        elif frameNumber >= self.memoryBufferStartSize and frameNumber < self.memoryBufferStartSize + self.explorationDecayFrames:\n",
    "            e = self.m*frameNumber + self.b\n",
    "        elif frameNumber >= self.memoryBufferStartSize + self.explorationDecayFrames:\n",
    "            e = self.m2*frameNumber + self.b2\n",
    "        elif inference:\n",
    "            e = self.explorationInference\n",
    "\n",
    "        if np.random.rand(1) < e:\n",
    "            return np.random.randint(0, env.action_space.n)\n",
    "        else:\n",
    "            return sess.run(mainDQN.bestAction, feed_dict={mainDQN.input:[state]})[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn():\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "    \"\"\"\n",
    "    states, actions, rewards, newStates, terminalFlags = myMemoryBuffer.getMinibatch()    \n",
    "    argQmax = sess.run(mainDQN.bestAction, feed_dict={mainDQN.input:newStates})\n",
    "    Qvals = sess.run(targetDQN.Qvalues, feed_dict={targetDQN.input:newStates})\n",
    "    \n",
    "    doubleQ = Qvals[range(bs), argQmax]\n",
    "    # Bellman equation\n",
    "    targetQ = rewards + (discountFactor*doubleQ * (1-terminalFlags))\n",
    "    _ = sess.run(mainDQN.update,feed_dict={mainDQN.input:states,mainDQN.targetQ:targetQ, mainDQN.action:actions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateGif(sess, frameNumber, framesForGif, reward):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "    \"\"\"\n",
    "    for idx,frame_idx in enumerate(framesForGif): \n",
    "        framesForGif[idx] = resize(frame_idx,(420,320,3),preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{PATH}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frameNumber, reward)}', framesForGif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control parameter\n",
    "maxEpisodeLength = 18000\n",
    "\n",
    "targetNetworkUpdateFreq = 10000\n",
    "discountFactor = 0.99\n",
    "memoryBufferStartSize = 5000\n",
    "maxFrames = 50000000\n",
    "memorySize = 1000000\n",
    "noOpSteps = 20\n",
    "gifFreq = 50\n",
    "\n",
    "hidden = 512\n",
    "learningRate = 0.00001\n",
    "bs = 32\n",
    "\n",
    "PATH = \"output/\"\n",
    "os.makedirs(PATH,exist_ok=True)\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "myMemoryBuffer = MemoryBuffer(size=memorySize, batchSize=bs)\n",
    "mainDQN = DQN(hidden, learningRate)\n",
    "targetDQN = DQN(hidden)\n",
    "variables = tf.trainable_variables()\n",
    "mainDQNVars = variables[0:len(variables)//2]\n",
    "targetDQNVars = variables[len(variables)//2:]\n",
    "\n",
    "NetworkUpdater = TargetNetworkUpdater(mainDQNVars, targetDQNVars)\n",
    "frameProcessor = processFrame()\n",
    "actionGetter = ActionGetter(memoryBufferStartSize=memoryBufferStartSize, maxFrames=maxFrames)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "restore = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if restore == True:\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "\n",
    "    frameNumber = 0\n",
    "    episodeNumber=0\n",
    "    rewards = []\n",
    "    \n",
    "    while frameNumber < maxFrames:\n",
    "        if episodeNumber % gifFreq == 0: \n",
    "            framesForGif = []\n",
    "        \n",
    "        frame = env.reset()\n",
    "        terminal = False\n",
    "        terminal2 = False\n",
    "        lastLives = 0\n",
    "        \n",
    "        # No op steps\n",
    "        for _ in range(random.randint(1, noOpSteps)):\n",
    "            frame, _, _, _ = env.step(0)\n",
    "            \n",
    "        processedFrame = frameProcessor.process(sess,frame)\n",
    "        state = np.repeat(processedFrame,4, axis=2)\n",
    "        episodeRewardSum = 0\n",
    "        \n",
    "        for j in range(maxEpisodeLength):\n",
    "            action = actionGetter.getAction(frameNumber,state)\n",
    "            newFrame, reward, terminal, info = env.step(action)\n",
    "            \n",
    "            # Pass terminal=True to Memory if live was lost\n",
    "            if info['ale.lives'] < lastLives:\n",
    "                terminal2 = True;\n",
    "            else:\n",
    "                terminal2 = terminal\n",
    "            lastLives = info['ale.lives']\n",
    "            \n",
    "            if episodeNumber % gifFreq == 0: \n",
    "                framesForGif.append(newFrame)\n",
    "                        \n",
    "            processedNewFrame = frameProcessor.process(sess,newFrame)\n",
    "            newState = np.append(state[:,:,1:],processedNewFrame,axis=2)\n",
    "\n",
    "            frameNumber += 1\n",
    "            \n",
    "            # Add current experience to Memory\n",
    "            myMemoryBuffer.addExperience(action=action, frame=processedNewFrame[:,:,0], reward=reward, terminal=terminal2)\n",
    "            \n",
    "            if frameNumber > memoryBufferStartSize:\n",
    "                learn()\n",
    "            \n",
    "            if frameNumber % targetNetworkUpdateFreq == 0 and frameNumber > memoryBufferStartSize:\n",
    "                NetworkUpdater.updateNetworks(sess)\n",
    "            \n",
    "            episodeRewardSum += reward\n",
    "            state = newState\n",
    "            \n",
    "            if terminal == True:\n",
    "                break\n",
    "                \n",
    "        rewards.append(episodeRewardSum)\n",
    "        if episodeNumber % gifFreq == 0: \n",
    "            generateGif(sess, frameNumber, framesForGif, episodeRewardSum)\n",
    "        if episodeNumber % gifFreq == 0:\n",
    "            saver.save(sess,PATH+'/my_model',global_step=frameNumber)\n",
    "        if episodeNumber % 10 == 0:\n",
    "            print(episodeNumber, frameNumber,np.mean(rewards[-100:]), j)\n",
    "            with open('rewards.dat','a') as f:\n",
    "                print(episodeNumber, frameNumber,np.mean(rewards[-100:]), j,file=f)\n",
    "        episodeNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()\n",
    "frameProcessor = processFrame()\n",
    "mainDQN = DQN(hidden, learningRate)\n",
    "targetDQN = DQN(hidden)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "    framesForGif = []\n",
    "    terminal = False\n",
    "    frame = env.reset()\n",
    "    processedFrame = frameProcessor.process(sess,frame)\n",
    "    state = np.repeat(processedFrame,4, axis=2)\n",
    "    episodeRewardSum = 0\n",
    "    \n",
    "    while not terminal:\n",
    "        action = getAction(1,state,inference=True)\n",
    "        newFrame, reward, terminal, _ = env.step(action)\n",
    "            \n",
    "        framesForGif.append(newFrame)\n",
    "                        \n",
    "        processedNewFrame = frameProcessor.process(sess,newFrame)\n",
    "        newState = np.append(state[:,:,1:],processedNewFrame,axis=2)\n",
    "\n",
    "\n",
    "        episodeRewardSum += reward\n",
    "        state = newState\n",
    "    print(\"Total reward: %s\" % episodeRewardSum)\n",
    "    generateGif(sess,0, framesForGif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
