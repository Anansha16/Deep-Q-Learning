{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"dqn/animations/\"\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(sess, frame_number, frames_for_gif):\n",
    "    \n",
    "    for idx,frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx,(420,320,3),preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{PATH}{\"pong_frame_{0}.gif\".format(frame_number)}', frames_for_gif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processFrame():\n",
    "    def __init__(self):\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    def process(self, sess, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return sess.run(self.processed, feed_dict={ self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn():\n",
    "    def __init__(self, hidden=512, learning_rate=0.00005):\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input = tf.placeholder(shape=[None,84,84,4], dtype=tf.float32)\n",
    "        self.inputscaled = self.input/255\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8,8], strides=4,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4,4], strides=2, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3,3], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv4 = tf.layers.conv2d(\n",
    "            inputs=self.conv3, filters=hidden, kernel_size=[7,7], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4,2,3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.Advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream,units=env.action_space.n,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.Value = tf.layers.dense(\n",
    "            inputs=self.valuestream,units=1,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.Qvalues = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keepdims=True))\n",
    "        self.bestaction = tf.argmax(self.Qvalues,1)\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qvalues, tf.one_hot(self.action, env.action_space.n, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.targetQ, predictions=self.Q))\n",
    "        #self.loss = tf.reduce_mean(tf.square(self.targetQ - self.Q))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)#0.0001\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getaction(frame_number,state, inference=False):\n",
    "    if frame_number < replay_start_size:\n",
    "        e = exploration_initial\n",
    "    if frame_number >= replay_start_size and frame_number < replay_start_size + exploration_decay_frames:\n",
    "        e = m*frame_number + b\n",
    "    if frame_number >= replay_start_size + exploration_decay_frames:\n",
    "        e = m2*frame_number + b2\n",
    "    if inference:\n",
    "        e = exploration_inference\n",
    "    if np.random.rand(1) < e:\n",
    "        return np.random.randint(0, env.action_space.n)\n",
    "    else:\n",
    "        return sess.run(mainDQN.bestaction, feed_dict={mainDQN.input:[state]})[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, size = 1000000, frame_height=84, frame_width=84, agent_history_length = 4, batch_size = 32):\n",
    "        \n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height,self.frame_width), dtype=np.uint8)\n",
    "        self.donestates = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the States and newStates in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.newstates = np.empty((self.batch_size, self.agent_history_length, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def addExperience(self, action, frame, reward, done):\n",
    "        \"\"\" \n",
    "        Adds an experience to the replay memory. Convention: array frames contains the newstates after the action\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current,...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.donestates[self.current] = done\n",
    "        \n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "        \n",
    "            \n",
    "    def getState(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1,...]\n",
    "        \n",
    "    def getValidIndices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.donestates[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def getMinibatch(self):\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        self.getValidIndices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self.getState(idx - 1)\n",
    "            self.newstates[i] = self.getState(idx)\n",
    "        return np.transpose(self.states,axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.newstates,axes=(0,2,3,1)), self.donestates[self.indices]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetVars_full(mainDQN_vars, targetDQN_vars):\n",
    "    update_ops = []\n",
    "    for i, var in enumerate(mainDQN_vars):\n",
    "        op = targetDQN_vars[i].assign(var.value())\n",
    "        update_ops.append(op)\n",
    "    return update_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    states, actions, rewards, newstates, dones = myMemoryBuffer.getMinibatch()    \n",
    "    argQmax = sess.run(mainDQN.bestaction, feed_dict={mainDQN.input:newstates})\n",
    "    Qvals = sess.run(targetDQN.Qvalues, feed_dict={targetDQN.input:newstates})\n",
    "    \n",
    "    done_mult = (1-dones)\n",
    "    doubleQ = Qvals[range(bs), argQmax]\n",
    "    targetQ = rewards + (gamma*doubleQ * done_mult)\n",
    "    _ = sess.run(mainDQN.update,feed_dict={mainDQN.input:states,mainDQN.targetQ:targetQ, mainDQN.action:actions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control parameter\n",
    "max_episode_len = 18000\n",
    "bs = 32\n",
    "target_network_update_freq = 10000\n",
    "gamma = 0.99\n",
    "exploration_initial = 1\n",
    "exploration_final = 0.1\n",
    "exploration_inference = 0.01\n",
    "exploration_decay_frames = 1000000#1000000\n",
    "replay_start_size = 5000\n",
    "max_frames = 4000000\n",
    "memory_size = 1000000 #1000000\n",
    "hidden = 512\n",
    "#interpol_factor = 0.001\n",
    "no_op_steps = 20\n",
    "gif_freq = 50\n",
    "learning_rate = 0.00005\n",
    "\n",
    "m = -(exploration_initial - exploration_final)/exploration_decay_frames\n",
    "b = exploration_initial - m*replay_start_size\n",
    "m2 = -(exploration_final - exploration_inference)/(max_frames - exploration_decay_frames - replay_start_size)\n",
    "b2 = exploration_final - m2*(replay_start_size + exploration_decay_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "myMemoryBuffer = MemoryBuffer(size=memory_size, batch_size=bs)\n",
    "mainDQN = dqn(hidden, learning_rate)\n",
    "targetDQN = dqn(hidden)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "restore = False\n",
    "\n",
    "\n",
    "variables = tf.trainable_variables()\n",
    "mainDQN_vars = variables[0:len(variables)//2]\n",
    "targetDQN_vars = variables[len(variables)//2:]\n",
    "updateTargetVars = updateTargetVars_full(mainDQN_vars, targetDQN_vars)\n",
    "frameprocessor = processFrame()\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if restore == True:\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "\n",
    "    frame_number = 0\n",
    "    episode_number=0\n",
    "    rewards = []\n",
    "    while frame_number < max_frames:\n",
    "        if episode_number % gif_freq == 0: \n",
    "            frames_for_gif = []\n",
    "        frame = env.reset()\n",
    "        done = False\n",
    "        for _ in range(random.randint(1, no_op_steps)):\n",
    "            frame, _, _, _ = env.step(0)\n",
    "        processed_frame = frameprocessor.process(sess,frame)\n",
    "        state = np.repeat(processed_frame,4, axis=2)\n",
    "        ep_reward_sum = 0\n",
    "        for j in range(max_episode_len):\n",
    "            action = getaction(frame_number,state)\n",
    "            newframe, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if episode_number % gif_freq == 0: \n",
    "                frames_for_gif.append(newframe)\n",
    "                        \n",
    "            processed_newframe = frameprocessor.process(sess,newframe)\n",
    "            newstate = np.append(state[:,:,1:],processed_newframe,axis=2)\n",
    "\n",
    "            frame_number += 1\n",
    "            \n",
    "            myMemoryBuffer.addExperience(action=action, frame=processed_newframe[:,:,0], reward=reward, done=done)\n",
    "            \n",
    "            if frame_number > replay_start_size:\n",
    "                learn()\n",
    "            \n",
    "            \n",
    "            if frame_number % target_network_update_freq == 0 and frame_number > replay_start_size:\n",
    "                update_ops = updateTargetVars\n",
    "                for op in update_ops:\n",
    "                    sess.run(op)\n",
    "            \n",
    "            \n",
    "            ep_reward_sum += reward\n",
    "            state = newstate\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards.append(ep_reward_sum)\n",
    "        if episode_number % gif_freq == 0: \n",
    "            generate_gif(sess, frame_number, frames_for_gif)\n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess,PATH+'/my_model',global_step=frame_number)\n",
    "        if episode_number % 10 == 0:\n",
    "            print(episode_number, frame_number,np.mean(rewards[-100:]), j)\n",
    "            with open('rewards.dat','a') as f:\n",
    "                print(episode_number, frame_number,np.mean(rewards[-100:]), j,file=f)\n",
    "        episode_number += 1\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "frameprocessor = processFrame()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "    frames_for_gif = []\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    processed_frame = frameprocessor.process(sess,frame)\n",
    "    state = np.repeat(processed_frame,4, axis=2)\n",
    "    ep_reward_sum = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = getaction(frame_number,state,inference=True)\n",
    "        newframe, reward, done, _ = env.step(action)\n",
    "            \n",
    "        frames_for_gif.append(newframe)\n",
    "                        \n",
    "        processed_newframe = frameprocessor.process(sess,newframe)\n",
    "        newstate = np.append(state[:,:,1:],processed_newframe,axis=2)\n",
    "\n",
    "\n",
    "        ep_reward_sum += reward\n",
    "        state = newstate\n",
    "    print(\"Total reward: %s\" % ep_reward_sum)\n",
    "    generate_gif(sess,0, frames_for_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
