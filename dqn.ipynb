{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"dqn/animations/\"\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-02 20:42:42,810] Making new env: PongDeterministic-v3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongDeterministic-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(sess, frame_number, frames_for_gif):\n",
    "    \n",
    "    for idx,frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx,(420,320,3),preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{PATH}{\"pong_frame_{0}.gif\".format(frame_number)}', frames_for_gif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processFrame():\n",
    "    def __init__(self):\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    def process(self, sess, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return sess.run(self.processed, feed_dict={ self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn():\n",
    "    def __init__(self, hidden=512, learning_rate=0.00005):\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input = tf.placeholder(shape=[None,84,84,4], dtype=tf.float32)\n",
    "        self.inputscaled = self.input/255\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8,8], strides=4,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4,4], strides=2, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3,3], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.conv4 = tf.layers.conv2d(\n",
    "            inputs=self.conv3, filters=hidden, kernel_size=[7,7], strides=1, \n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False)\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4,2,3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.Advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream,units=env.action_space.n,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.Value = tf.layers.dense(\n",
    "            inputs=self.valuestream,units=1,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.Qvalues = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keepdims=True))\n",
    "        self.bestaction = tf.argmax(self.Qvalues,1)\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qvalues, tf.one_hot(self.action, env.action_space.n, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.targetQ, predictions=self.Q))\n",
    "        #self.loss = tf.reduce_mean(tf.square(self.targetQ - self.Q))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)#0.0001\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getaction(frame_number,state, inference=False):\n",
    "    if frame_number < replay_start_size:\n",
    "        e = exploration_initial\n",
    "    if frame_number >= replay_start_size and frame_number < replay_start_size + exploration_decay_frames:\n",
    "        e = m*frame_number + b\n",
    "    if frame_number >= replay_start_size + exploration_decay_frames:\n",
    "        e = m2*frame_number + b2\n",
    "    if inference:\n",
    "        e = exploration_inference\n",
    "    if np.random.rand(1) < e:\n",
    "        return np.random.randint(0, env.action_space.n)\n",
    "    else:\n",
    "        return sess.run(mainDQN.bestaction, feed_dict={mainDQN.input:[state]})[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, size = 1000000, frame_height=84, frame_width=84, agent_history_length = 4, batch_size = 32):\n",
    "        \n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height,self.frame_width), dtype=np.uint8)\n",
    "        self.donestates = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the States and newStates in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.newstates = np.empty((self.batch_size, self.agent_history_length, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def addExperience(self, action, frame, reward, done):\n",
    "        \"\"\" \n",
    "        Adds an experience to the replay memory. Convention: array frames contains the newstates after the action\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current,...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.donestates[self.current] = done\n",
    "        \n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "        \n",
    "            \n",
    "    def getState(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1,...]\n",
    "        \n",
    "    def getValidIndices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.donestates[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def getMinibatch(self):\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        self.getValidIndices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self.getState(idx - 1)\n",
    "            self.newstates[i] = self.getState(idx)\n",
    "        return np.transpose(self.states,axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.newstates,axes=(0,2,3,1)), self.donestates[self.indices]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetVars_full(mainDQN_vars, targetDQN_vars):\n",
    "    update_ops = []\n",
    "    for i, var in enumerate(mainDQN_vars):\n",
    "        op = targetDQN_vars[i].assign(var.value())\n",
    "        update_ops.append(op)\n",
    "    return update_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    states, actions, rewards, newstates, dones = myMemoryBuffer.getMinibatch()    \n",
    "    argQmax = sess.run(mainDQN.bestaction, feed_dict={mainDQN.input:newstates})\n",
    "    Qvals = sess.run(targetDQN.Qvalues, feed_dict={targetDQN.input:newstates})\n",
    "    \n",
    "    done_mult = (1-dones)\n",
    "    doubleQ = Qvals[range(bs), argQmax]\n",
    "    targetQ = rewards + (gamma*doubleQ * done_mult)\n",
    "    _ = sess.run(mainDQN.update,feed_dict={mainDQN.input:states,mainDQN.targetQ:targetQ, mainDQN.action:actions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control parameter\n",
    "max_episode_len = 18000\n",
    "bs = 32\n",
    "target_network_update_freq = 10000\n",
    "gamma = 0.99\n",
    "exploration_initial = 1\n",
    "exploration_final = 0.1\n",
    "exploration_inference = 0.01\n",
    "exploration_decay_frames = 1000000#1000000\n",
    "replay_start_size = 5000\n",
    "max_frames = 4000000\n",
    "memory_size = 1000000 #1000000\n",
    "hidden = 512\n",
    "#interpol_factor = 0.001\n",
    "no_op_steps = 20\n",
    "gif_freq = 50\n",
    "learning_rate = 0.00005\n",
    "\n",
    "m = -(exploration_initial - exploration_final)/exploration_decay_frames\n",
    "b = exploration_initial - m*replay_start_size\n",
    "m2 = -(exploration_final - exploration_inference)/(max_frames - exploration_decay_frames - replay_start_size)\n",
    "b2 = exploration_final - m2*(replay_start_size + exploration_decay_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiograetz/anaconda3/envs/deeplearning/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/fabiograetz/anaconda3/envs/deeplearning/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9275 -20.3 841\n",
      "19 18421 -20.25 986\n",
      "29 27696 -20.233333333333334 998\n",
      "39 36833 -20.3 929\n",
      "49 45794 -20.3 901\n",
      "59 54756 -20.316666666666666 966\n",
      "69 63371 -20.4 942\n",
      "79 72514 -20.375 810\n",
      "89 81057 -20.41111111111111 815\n",
      "99 90447 -20.38 1118\n",
      "109 100467 -20.31 975\n",
      "119 110545 -20.26 1062\n",
      "129 120117 -20.25 978\n",
      "139 130750 -20.19 1232\n",
      "149 139977 -20.19 803\n",
      "159 150291 -20.15 1071\n",
      "169 160483 -20.03 1003\n",
      "179 170848 -20.0 994\n",
      "189 180743 -19.96 882\n",
      "199 191543 -19.9 849\n",
      "209 201984 -19.95 973\n",
      "219 213899 -19.92 841\n",
      "229 224425 -19.85 929\n",
      "239 235457 -19.85 1019\n",
      "249 246118 -19.76 1061\n",
      "259 255950 -19.81 1085\n",
      "269 265328 -19.9 939\n",
      "279 275778 -19.89 1084\n",
      "289 286577 -19.83 1034\n",
      "299 297421 -19.84 913\n",
      "309 307400 -19.87 1099\n",
      "319 318029 -19.91 1069\n",
      "329 329397 -19.93 1149\n",
      "339 340969 -19.9 919\n",
      "349 352166 -19.9 938\n",
      "359 364893 -19.72 1126\n",
      "369 375260 -19.66 1007\n",
      "379 386195 -19.65 1238\n",
      "389 397383 -19.6 1222\n",
      "399 409664 -19.57 1417\n",
      "409 420756 -19.48 1106\n",
      "419 431938 -19.46 1005\n",
      "429 444824 -19.37 1254\n",
      "439 456446 -19.34 1018\n",
      "449 468148 -19.38 1138\n",
      "459 481601 -19.34 2019\n",
      "469 496483 -19.08 1724\n",
      "479 510991 -18.95 1297\n",
      "489 527118 -18.77 1325\n",
      "499 542634 -18.55 1166\n",
      "509 559475 -18.23 1600\n",
      "519 575321 -17.96 1882\n",
      "529 591997 -17.75 1388\n",
      "539 611594 -17.29 2526\n",
      "549 630679 -16.86 2591\n",
      "559 650820 -16.44 2017\n",
      "569 673119 -15.88 2689\n",
      "579 695720 -15.3 2030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c1f838e413bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mreplay_start_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-14da95aa2e18>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyMemoryBuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMinibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margQmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnewstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mQvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnewstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdone_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "myMemoryBuffer = MemoryBuffer(size=memory_size, batch_size=bs)\n",
    "mainDQN = dqn(hidden, learning_rate)\n",
    "targetDQN = dqn(hidden)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "restore = False\n",
    "\n",
    "\n",
    "variables = tf.trainable_variables()\n",
    "mainDQN_vars = variables[0:len(variables)//2]\n",
    "targetDQN_vars = variables[len(variables)//2:]\n",
    "updateTargetVars = updateTargetVars_full(mainDQN_vars, targetDQN_vars)\n",
    "frameprocessor = processFrame()\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if restore == True:\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "\n",
    "    frame_number = 0\n",
    "    episode_number=0\n",
    "    rewards = []\n",
    "    while frame_number < max_frames:\n",
    "        if episode_number % gif_freq == 0: \n",
    "            frames_for_gif = []\n",
    "        frame = env.reset()\n",
    "        done = False\n",
    "        for _ in range(random.randint(1, no_op_steps)):\n",
    "            frame, _, _, _ = env.step(0)\n",
    "        processed_frame = frameprocessor.process(sess,frame)\n",
    "        state = np.repeat(processed_frame,4, axis=2)\n",
    "        ep_reward_sum = 0\n",
    "        for j in range(max_episode_len):\n",
    "            action = getaction(frame_number,state)\n",
    "            newframe, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if episode_number % gif_freq == 0: \n",
    "                frames_for_gif.append(newframe)\n",
    "                        \n",
    "            processed_newframe = frameprocessor.process(sess,newframe)\n",
    "            newstate = np.append(state[:,:,1:],processed_newframe,axis=2)\n",
    "\n",
    "            frame_number += 1\n",
    "            \n",
    "            myMemoryBuffer.addExperience(action=action, frame=processed_newframe[:,:,0], reward=reward, done=done)\n",
    "            \n",
    "            if frame_number > replay_start_size:\n",
    "                learn()\n",
    "            \n",
    "            \n",
    "            if frame_number % target_network_update_freq == 0 and frame_number > replay_start_size:\n",
    "                update_ops = updateTargetVars\n",
    "                for op in update_ops:\n",
    "                    sess.run(op)\n",
    "            \n",
    "            \n",
    "            ep_reward_sum += reward\n",
    "            state = newstate\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards.append(ep_reward_sum)\n",
    "        if episode_number % gif_freq == 0: \n",
    "            generate_gif(sess, frame_number, frames_for_gif)\n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess,PATH+'/my_model',global_step=frame_number)\n",
    "        if episode_number % 10 == 0:\n",
    "            print(episode_number, frame_number,np.mean(rewards[-100:]), j)\n",
    "            with open('rewards.dat','a') as f:\n",
    "                print(episode_number, frame_number,np.mean(rewards[-100:]), j,file=f)\n",
    "        episode_number += 1\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "frameprocessor = processFrame()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
    "    frames_for_gif = []\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    processed_frame = frameprocessor.process(sess,frame)\n",
    "    state = np.repeat(processed_frame,4, axis=2)\n",
    "    ep_reward_sum = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = getaction(frame_number,state,inference=True)\n",
    "        newframe, reward, done, _ = env.step(action)\n",
    "            \n",
    "        frames_for_gif.append(newframe)\n",
    "                        \n",
    "        processed_newframe = frameprocessor.process(sess,newframe)\n",
    "        newstate = np.append(state[:,:,1:],processed_newframe,axis=2)\n",
    "\n",
    "\n",
    "        ep_reward_sum += reward\n",
    "        state = newstate\n",
    "    print(\"Total reward: %s\" % ep_reward_sum)\n",
    "    generate_gif(sess,0, frames_for_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
